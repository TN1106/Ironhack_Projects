{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('OnlineNewsPopularity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>...</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://mashable.com/2013/01/07/amazon-instant-...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>0.663594</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.350000</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://mashable.com/2013/01/07/ap-samsung-spon...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.604743</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.791946</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.118750</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://mashable.com/2013/01/07/apple-40-billio...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>0.575130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.466667</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://mashable.com/2013/01/07/astronaut-notre...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>0.503788</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.665635</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.369697</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://mashable.com/2013/01/07/att-u-verse-apps/</td>\n",
       "      <td>731.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>0.415646</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.540890</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.220192</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url   timedelta  \\\n",
       "0  http://mashable.com/2013/01/07/amazon-instant-...       731.0   \n",
       "1  http://mashable.com/2013/01/07/ap-samsung-spon...       731.0   \n",
       "2  http://mashable.com/2013/01/07/apple-40-billio...       731.0   \n",
       "3  http://mashable.com/2013/01/07/astronaut-notre...       731.0   \n",
       "4   http://mashable.com/2013/01/07/att-u-verse-apps/       731.0   \n",
       "\n",
       "    n_tokens_title   n_tokens_content   n_unique_tokens   n_non_stop_words  \\\n",
       "0             12.0              219.0          0.663594                1.0   \n",
       "1              9.0              255.0          0.604743                1.0   \n",
       "2              9.0              211.0          0.575130                1.0   \n",
       "3              9.0              531.0          0.503788                1.0   \n",
       "4             13.0             1072.0          0.415646                1.0   \n",
       "\n",
       "    n_non_stop_unique_tokens   num_hrefs   num_self_hrefs   num_imgs  ...  \\\n",
       "0                   0.815385         4.0              2.0        1.0  ...   \n",
       "1                   0.791946         3.0              1.0        1.0  ...   \n",
       "2                   0.663866         3.0              1.0        1.0  ...   \n",
       "3                   0.665635         9.0              0.0        1.0  ...   \n",
       "4                   0.540890        19.0             19.0       20.0  ...   \n",
       "\n",
       "    min_positive_polarity   max_positive_polarity   avg_negative_polarity  \\\n",
       "0                0.100000                     0.7               -0.350000   \n",
       "1                0.033333                     0.7               -0.118750   \n",
       "2                0.100000                     1.0               -0.466667   \n",
       "3                0.136364                     0.8               -0.369697   \n",
       "4                0.033333                     1.0               -0.220192   \n",
       "\n",
       "    min_negative_polarity   max_negative_polarity   title_subjectivity  \\\n",
       "0                  -0.600               -0.200000             0.500000   \n",
       "1                  -0.125               -0.100000             0.000000   \n",
       "2                  -0.800               -0.133333             0.000000   \n",
       "3                  -0.600               -0.166667             0.000000   \n",
       "4                  -0.500               -0.050000             0.454545   \n",
       "\n",
       "    title_sentiment_polarity   abs_title_subjectivity  \\\n",
       "0                  -0.187500                 0.000000   \n",
       "1                   0.000000                 0.500000   \n",
       "2                   0.000000                 0.500000   \n",
       "3                   0.000000                 0.500000   \n",
       "4                   0.136364                 0.045455   \n",
       "\n",
       "    abs_title_sentiment_polarity   shares  \n",
       "0                       0.187500      593  \n",
       "1                       0.000000      711  \n",
       "2                       0.000000     1500  \n",
       "3                       0.000000     1200  \n",
       "4                       0.136364      505  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for missing numbers\n",
    "\n",
    "missing_values = df[df.isnull().any(axis=1)]\n",
    "missing_values.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(missing_values.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There are no missing values, and the data set appears to be super clean, with dummies already included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reverse-clean for visualization purposes\n",
    "df_rev = df.copy()\n",
    "df_rev.columns = df_rev.columns.str.replace(' ','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev['Channel'] = 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_channel = {'data_channel_is_lifestyle':'Lifestyle','data_channel_is_entertainment':'Entertainment',\n",
    "               'data_channel_is_bus':'Business',\n",
    "               'data_channel_is_socmed':'Social_Media', 'data_channel_is_tech':\"Tech\",\n",
    "               'data_channel_is_world':'World'}\n",
    "\n",
    "for i in col_channel.keys():\n",
    "    df_rev['Channel'] = np.where(df_rev[i]==1.0, col_channel[i],df_rev['Channel'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev[df_rev['Channel']=='0']['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev['Channel'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_channel = {'weekday_is_monday':'Monday', 'weekday_is_tuesday':'Tuesday',\n",
    "       'weekday_is_wednesday':'Wednesday', 'weekday_is_thursday':'Thursday', 'weekday_is_friday':'Friday',\n",
    "       'weekday_is_saturday':'Saturday', 'weekday_is_sunday':'Sunday'}\n",
    "\n",
    "df_rev['Day_of_Week'] = None\n",
    "\n",
    "for i in day_channel.keys():\n",
    "    df_rev['Day_of_Week'] = np.where(df_rev[i]==1, day_channel[i],df_rev['Day_of_Week'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_drop = ['weekday_is_monday', 'weekday_is_tuesday',\n",
    "       'weekday_is_wednesday', 'weekday_is_thursday', 'weekday_is_friday',\n",
    "       'weekday_is_saturday', 'weekday_is_sunday','data_channel_is_lifestyle',\n",
    "       'data_channel_is_entertainment', 'data_channel_is_bus',\n",
    "       'data_channel_is_socmed', 'data_channel_is_tech',\n",
    "       'data_channel_is_world']\n",
    "\n",
    "df_rev.drop(columns=cols_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "sns.heatmap(df.corr(),cmap=\"coolwarm\")\n",
    "plt.title('Correlation matrix',fontsize=24)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev['Channel'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_C = df_rev.groupby(['Channel'])['shares'].agg(['mean','sum',np.size]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_C.set_index('Channel',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_C.sort_values(by='mean',inplace=True, ascending=False)\n",
    "df_agg_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross = pd.crosstab(df_rev.Channel,df_rev.Day_of_Week)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors1 = sns.color_palette(\"GnBu_d\", len(df_agg_C.index))\n",
    "colors2 = sns.color_palette(\"BuGn_d\", len(df_agg_C.index))\n",
    "\n",
    "fig, [ax1, ax2] = plt.subplots(1,2, figsize=(20,5))\n",
    "df_agg_C = df_agg_C[df_agg_C.index != 'Other']\n",
    "\n",
    "ax = sns.barplot(y = df_agg_C.index, x = df_agg_C['mean'], orient='h', palette = colors1, ax=ax1)\n",
    "ax = sns.barplot(y = df_agg_C.index, x = df_agg_C['size'], orient='h', palette = colors2, ax=ax2)\n",
    "\n",
    "ax1.set_xlabel(xlabel='Average Number of Shares per Article', fontsize=16)\n",
    "ax1.set_ylabel(ylabel=\"Channel\", fontsize=16)\n",
    "ax1.set_title(label=\"Average Number of Shares on Social per Article, by Channel\", fontsize=22)\n",
    "\n",
    "ax2.set_xlabel(xlabel='Number of Articles', fontsize=16)\n",
    "ax2.set_ylabel(ylabel=\"Channel\", fontsize=16)\n",
    "ax2.set_title(label=\"Count of Articles\", fontsize=22)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg2 = df_rev.groupby(['Day_of_Week'])['shares'].agg(['mean','sum',np.size])\n",
    "df_agg2 = df_agg2.sort_values('mean',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors1 = sns.color_palette(\"Blues_d\", len(df_agg2.index))\n",
    "colors2 = sns.color_palette(\"BuGn_d\", len(df_agg2.index))\n",
    "\n",
    "fig, [ax1, ax2] = plt.subplots(1,2, figsize=(20,5))\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "\n",
    "ax = sns.barplot(y = df_agg2.index, x = df_agg2['mean'], orient='h', palette = colors1, ax=ax1)\n",
    "ax = sns.barplot(y = df_agg2.index, x = df_agg2['size'], orient='h', palette = colors2, ax=ax2)\n",
    "\n",
    "ax1.set_xlabel(xlabel='Average Number of Sharesper Article, by Day of Week Released', fontsize=16)\n",
    "ax1.set_ylabel(ylabel=\"Day\", fontsize=16)\n",
    "ax1.set_title(label=\"Average Number of Shares on Social per Article\", fontsize=24)\n",
    "\n",
    "ax2.set_xlabel(xlabel='Number of Articles by Day Released', fontsize=16)\n",
    "ax2.set_ylabel(ylabel=\"Day\", fontsize=16)\n",
    "ax2.set_title(label=\"Count of Articles\", fontsize=24)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cross = cross[['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']]\n",
    "plt.figure(figsize=(16,8))\n",
    "sns.heatmap(cross,cmap=\"viridis\", annot=True, fmt=\"d\")\n",
    "plt.title('Total Number of Articles',fontsize=24)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_rev1 = df_rev[df_rev['Channel']!='Other']\n",
    "\n",
    "ch = ['Entertainment','Business','Tech','Lifestyle','World','Social_Media']\n",
    "\n",
    "\n",
    "medians = df_rev1.groupby(['Channel'])['shares'].median()\n",
    "medians = medians[ch].values\n",
    "\n",
    "median_labels = [str(np.round(s, 2)) for s in medians]\n",
    "\n",
    "\n",
    "means = df_rev1.groupby(['Channel'])['shares'].mean()\n",
    "means = means[ch].values\n",
    "mean_labels = [str(np.round(s, 2)) for s in means]\n",
    "\n",
    "plt.figure(figsize=(18,9))\n",
    "\n",
    "ax = sns.boxplot(x=df_rev1['Channel'], y=df_rev1['shares'],data=df_rev1, hue='is_weekend', palette='RdPu', \n",
    "                 showfliers=False, showmeans=True, linewidth=3.)\n",
    "ax.set_ylabel(ylabel='# Shares on Social', fontsize=20)\n",
    "ax.set_title(label='Article Shares by Channel', fontsize=20)\n",
    "\n",
    "leg = ax.get_legend()\n",
    "\n",
    "new_title = 'Weekend?'\n",
    "leg.set_title(new_title)\n",
    "new_labels = ['No', 'Yes']\n",
    "for t, l in zip(leg.texts, new_labels): t.set_text(l)\n",
    "    \n",
    "plt.setp(ax.get_legend().get_texts(), fontsize='18') # for legend text\n",
    "plt.setp(ax.get_legend().get_title(), fontsize='22') # for legend title\n",
    "\n",
    "pos = range(len(medians))\n",
    "for tick,label in zip(pos,ax.get_xticklabels()):\n",
    "    ax.text(pos[tick], medians[tick], f'med={median_labels[tick]}', \n",
    "            horizontalalignment='center', size='x-small', color='c', weight='semibold',fontsize=20)\n",
    "    ax.text(pos[tick], means[tick], f'mean={mean_labels[tick]}', \n",
    "            horizontalalignment='center', size='x-small', color='g', weight='bold',fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "\n",
    "\n",
    "\n",
    "ax = sns.scatterplot(x='shares',y='n_tokens_content',data=df_rev, hue='is_weekend') #Hue\n",
    "ax.set_ylabel(ylabel='Article Word Count', fontsize=16)\n",
    "ax.set_xlabel(xlabel='Number of Shares', fontsize=16)\n",
    "ax.set_title(label='Shares by Length of Article', fontsize=20)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "\n",
    "\n",
    "\n",
    "ax = sns.scatterplot(x='shares',y='num_videos',data=df_rev, hue='is_weekend') #Hue\n",
    "ax.set_ylabel(ylabel='Article Video Count', fontsize=16)\n",
    "ax.set_xlabel(xlabel='Number of Shares', fontsize=16)\n",
    "ax.set_title(label='Shares by Number of Videos', fontsize=20)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "\n",
    "\n",
    "\n",
    "ax = sns.scatterplot(x='shares',y='num_imgs',data=df_rev, hue='is_weekend') #Hue\n",
    "ax.set_ylabel(ylabel='Article Image Count', fontsize=16)\n",
    "ax.set_xlabel(xlabel='Number of Shares', fontsize=16)\n",
    "ax.set_title(label='Shares by Number of Images', fontsize=20)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev['num_imgs'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def iqr_fun():\n",
    "    \n",
    "for i in ['shares']:\n",
    "    Q25 = df_rev[i].quantile(0.25)\n",
    "    Q75 = df_rev[i].quantile(0.75)\n",
    "    IQR = Q75-Q25\n",
    "    sMaxQ = Q75+1.5*IQR\n",
    "    sMinQ = max(Q25-1.5*IQR,0.0)\n",
    "    print(sMinQ, sMaxQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_revO = df_rev[(df_rev['shares']<MaxQ) & (df_rev['shares']>MinQ)  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev['shares'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "\n",
    "df1 = df_rev[(df_rev['shares']<sMaxQ) & (df_rev['shares']>sMinQ)  ]\n",
    "\n",
    "ax= df1['shares'].hist(bins=54)\n",
    "   \n",
    "ax.set_ylabel('Articles', fontsize=20)\n",
    "ax.set_xlabel('Number of Shares', fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Words in Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def iqr_fun():\n",
    "    \n",
    "for i in ['n_tokens_content']:\n",
    "    Q25 = df_rev[i].quantile(0.25)\n",
    "    Q75 = df_rev[i].quantile(0.75)\n",
    "    IQR = Q75-Q25\n",
    "    MaxQ = Q75+1.5*IQR\n",
    "    MinQ = max(Q25-1.5*IQR,0.0)\n",
    "    print(MinQ, MaxQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_revO = df_rev[(df_rev['n_tokens_content']<MaxQ) & (df_rev['n_tokens_content']>MinQ)  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "\n",
    "\n",
    "ax= df_revO['n_tokens_content'].hist(bins=54)\n",
    "   \n",
    "ax.set_ylabel('Articles', fontsize=20)\n",
    "ax.set_xlabel('Number of Words', fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['num_videos']:\n",
    "    Q25 = df_rev[i].quantile(0.25)\n",
    "    Q75 = df_rev[i].quantile(0.75)\n",
    "    IQR = Q75-Q25\n",
    "    MaxQ = Q75+2*IQR\n",
    "    MinQ = max(Q25-1.5*IQR,0.0)\n",
    "    print(MinQ, MaxQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_revO = df_revO[(df_revO['num_videos']<=MaxQ) & (df_revO['num_videos']>=MinQ)  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_revO.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "\n",
    "ax= df_revO['num_videos'].hist(bins=4)\n",
    "   \n",
    "ax.set_ylabel('Articles', fontsize=20)\n",
    "ax.set_xlabel('Number of Videos', fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop Model for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.replace(' ','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = df.copy()\n",
    "\n",
    "def iqr_fun(i,dff):\n",
    "    Q25 = dff[i].quantile(0.25)\n",
    "    Q75 = dff[i].quantile(0.75)\n",
    "    IQR = Q75-Q25\n",
    "    MaxQ = round(Q75+1.5*IQR,0)+1\n",
    "    MinQ = round((Q25-1.5*IQR),0)-1\n",
    "    print(MinQ, MaxQ)\n",
    "    return [MinQ, MaxQ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dff['kw_min_min'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_out = ['n_tokens_title','n_tokens_content','num_hrefs','num_self_hrefs','num_imgs','num_videos','num_keywords',\n",
    "              'kw_min_min', 'kw_max_min',\"kw_avg_min\", \"kw_min_max\", \"kw_max_max\",\"kw_avg_max\", \"kw_min_avg\",\n",
    "               \"kw_max_avg\",\"kw_avg_avg\",\"self_reference_min_shares\",\"self_reference_max_shares\",\n",
    "               \"self_reference_avg_sharess\"]\n",
    "\n",
    "for n in columns_out:\n",
    "    print(n)\n",
    "    \n",
    "    q = iqr_fun(n,dff)\n",
    "    print(q)\n",
    "    \n",
    "    dff = dff[(dff[n]<=q[1]) & (dff[n]>=q[0])  ]\n",
    "    \n",
    "    #print(dff.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorize Shares into 3 Bins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = dff[dff.shares<=sMaxQ+1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.shares.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.drop(['url','timedelta'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser, bins = pd.qcut(dff[\"shares\"], [0, 0.333,0.6667,1], retbins=True, labels=['poor','good','viral'])\n",
    "\n",
    "#ser, bins = pd.qcut(dff[\"shares\"], 4, retbins=True, labels=['poor','good','great','viral'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dff['share_cat'] = pd.cut(dff[\"shares\"], bins=bins, labels=['poor','good','great','viral'], include_lowest=True)\n",
    "dff['share_cat'] = pd.cut(dff[\"shares\"], bins=bins, labels=[0,1,2], include_lowest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.drop(['shares'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_tr1, X_t1, y_tr1,y_t1 = train_test_split(dff.drop('share_cat',axis=1), dff['share_cat'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log2_model = LogisticRegression(solver='liblinear')\n",
    "\n",
    "log2_model.fit(X_tr1,y_tr1)\n",
    "\n",
    "y_p_tr1= log2_model.predict(X_tr1)\n",
    "y_p_t1= log2_model.predict(X_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "acc_score_tr1 = accuracy_score(y_tr1, y_p_tr1)\n",
    "acc_score_t1 = accuracy_score(y_t1, y_p_t1)\n",
    "\n",
    "print(\"Acc Score on traning set: {}\".format(acc_score_tr1))\n",
    "print(\"Acc Score on testing set: {}\".format(acc_score_t1)) \n",
    "print(\"\")\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "bacc_score_tr1 = balanced_accuracy_score(y_tr1, y_p_tr1)\n",
    "bacc_score_t1 = balanced_accuracy_score(y_t1, y_p_t1)\n",
    "\n",
    "print(\"Balanced Acc Score on traning set: {}\".format(bacc_score_tr1))\n",
    "print(\"Balanced Acc Score on testing set: {}\".format(bacc_score_t1)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_t1, y_p_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model is bad. Try first doing some feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log2_model2 = LogisticRegression(multi_class='multinomial', solver='newton-cg') #lbfgs/liblinear is default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe1 = RFE(log2_model2,n_features_to_select=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rfe1.fit(X_tr1,y_tr1)\n",
    "print(rfe1.ranking_)\n",
    "print(rfe1.support_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = X_tr1.columns[rfe1.support_]\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_tr2, X_t2, y_tr2, y_t2 = train_test_split(dff[cols], dff['share_cat'], test_size=0.20, \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log2_model2.fit(X_tr2,y_tr2)\n",
    "\n",
    "y_p_tr2= log2_model2.predict(X_tr2)\n",
    "y_p_t2= log2_model2.predict(X_t2)\n",
    "\n",
    "acc_score_tr2 = accuracy_score(y_tr2, y_p_tr2)\n",
    "acc_score_t2 = accuracy_score(y_t2, y_p_t2)\n",
    "\n",
    "print(\"Acc Score on traning set: {}\".format(acc_score_tr2))\n",
    "print(\"Acc Score on testing set: {}\".format(acc_score_t2)) \n",
    "print(\"\")\n",
    "\n",
    "bacc_score_tr2 = balanced_accuracy_score(y_tr2, y_p_tr2)\n",
    "bacc_score_t2 = balanced_accuracy_score(y_t2, y_p_t2)\n",
    "\n",
    "print(\"Balanced Acc Score on traning set: {}\".format(bacc_score_tr2))\n",
    "print(\"Balanced Acc Score on testing set: {}\".format(bacc_score_t2)) \n",
    "print(\"\")\n",
    "\n",
    "precision_tr2 = precision_score(y_tr2, y_p_tr2, average='micro')\n",
    "precision_t2 = precision_score(y_t2, y_p_t2,average='micro')\n",
    "\n",
    "print(\"Precision Score on training set: {}\".format(precision_tr2))\n",
    "print(\"Precision Score on testing set: {}\".format(precision_t2)) \n",
    "print(\"\")\n",
    "\n",
    "recall_tr2 = recall_score(y_tr2, y_p_tr2, average='micro')\n",
    "recall_t2 = recall_score(y_t2, y_p_t2, average='micro')\n",
    "\n",
    "print(\"Recall Score on training set: {}\".format(recall_tr2))\n",
    "print(\"Recall Score on testing set: {}\".format(recall_t2)) \n",
    "print(\"\")\n",
    "\n",
    "f1_score_tr = f1_score(y_tr2, y_p_tr2, average='micro')\n",
    "f1_score_t = f1_score(y_t2, y_p_t2, average='micro')\n",
    "               \n",
    "print(\"F1 Score on training set: {}\".format(f1_score_tr))\n",
    "print(\"F1 Score on testing set: {}\".format(f1_score_t)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_t2, y_p_t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_t2, y_p_t2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if we eliminated NO outliers..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy()\n",
    "df1.drop(['url','timedelta'],axis=1,inplace=True)\n",
    "ser, bins = pd.qcut(df1[\"shares\"], [0,0.3333,0.6667,1.0], retbins=True, labels=['poor','good','viral'])\n",
    "df1['share_cat'] = pd.cut(df1[\"shares\"], bins=bins, labels=[0,1,2], include_lowest=True)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.drop(['shares'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr3a, X_t3a, y_tr3a, y_t3a = train_test_split(df1.drop('share_cat',axis=1), df1['share_cat'], test_size=0.20, \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model3 = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "rfe3 = RFE(log_model3,n_features_to_select=6)\n",
    "rfe3.fit(X_tr3a,y_tr3a)\n",
    "print(rfe3.ranking_)\n",
    "print(rfe3.support_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = X_tr3a.columns[rfe3.support_]\n",
    "X_tr3, X_t3, y_tr3, y_t3 = train_test_split(df1[cols], df1['share_cat'], test_size=0.20, \n",
    "                                                    random_state=0)\n",
    "\n",
    "\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model3.fit(X_tr3,y_tr3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p_tr3= log_model3.predict(X_tr3)\n",
    "y_p_t3= log_model3.predict(X_t3)\n",
    "\n",
    "acc_score_tr3 = accuracy_score(y_tr3, y_p_tr3)\n",
    "acc_score_t3 = accuracy_score(y_t3, y_p_t3)\n",
    "\n",
    "print(\"Acc Score on traning set: {}\".format(acc_score_tr3))\n",
    "print(\"Acc Score on testing set: {}\".format(acc_score_t3)) \n",
    "print(\"\")\n",
    "\n",
    "bacc_score_tr3 = balanced_accuracy_score(y_tr3, y_p_tr3)\n",
    "bacc_score_t3 = balanced_accuracy_score(y_t3, y_p_t3)\n",
    "\n",
    "print(\"Balanced Acc Score on traning set: {}\".format(bacc_score_tr3))\n",
    "print(\"Balanced Acc Score on testing set: {}\".format(bacc_score_t3)) \n",
    "print(\"\")\n",
    "\n",
    "precision_tr3 = precision_score(y_tr3, y_p_tr3, average='micro')\n",
    "precision_t3 = precision_score(y_t3, y_p_t3, average='micro')\n",
    "\n",
    "print(\"Precision Score on training set: {}\".format(precision_tr3))\n",
    "print(\"Precision Score on testing set: {}\".format(precision_t3)) \n",
    "print(\"\")\n",
    "\n",
    "recall_tr3 = recall_score(y_tr3, y_p_tr3, average='micro')\n",
    "recall_t3 = recall_score(y_t3, y_p_t3, average='micro')\n",
    "\n",
    "print(\"Recall Score on training set: {}\".format(recall_tr3))\n",
    "print(\"Recall Score on testing set: {}\".format(recall_t3)) \n",
    "print(\"\")\n",
    "\n",
    "f1_score_tr = f1_score(y_tr3, y_p_tr3, average='micro')\n",
    "f1_score_t = f1_score(y_t3, y_p_t3, average='micro')\n",
    "               \n",
    "print(\"F1 Score on training set: {}\".format(f1_score_tr))\n",
    "print(\"F1 Score on testing set: {}\".format(f1_score_t)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_t3, y_p_t3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_t3, y_p_t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "ks_rf = RandomForestClassifier().fit(X_tr3,y_tr3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p_t_RF = ks_rf.predict(X_t3)\n",
    "confusion_matrix(y_t3,y_p_t_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest with Outliers Removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()\n",
    "\n",
    "columns_out = ['n_tokens_title','n_tokens_content','num_hrefs','num_self_hrefs','num_imgs','num_videos','num_keywords',\n",
    "              'kw_min_min', 'kw_max_min',\"kw_avg_min\", \"kw_min_max\", \"kw_max_max\",\"kw_avg_max\", \"kw_min_avg\",\n",
    "               \"kw_max_avg\",\"kw_avg_avg\",\"self_reference_min_shares\",\"self_reference_max_shares\",\n",
    "               \"self_reference_avg_sharess\"]\n",
    "\n",
    "for n in columns_out:\n",
    "    print(n)\n",
    "    \n",
    "    q = iqr_fun(n,df2)\n",
    "    df2 = df2[(df2[n]<=q[1]) & (df2[n]>=q[0])  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2[df2.shares<=sMaxQ+1000]\n",
    "df2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "ser, bins = pd.qcut(df2[\"shares\"], [0,0.333,0.6667,1], retbins=True, labels=['poor','good','viral'])\n",
    "df2['share_cat'] = pd.cut(df2[\"shares\"], bins=bins, labels=[0,1,2], include_lowest=True)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.drop(['shares'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr4a, X_t4a, y_tr4a,y_t4a = train_test_split(df2.drop('share_cat',axis=1), df2['share_cat'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model4 = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "\n",
    "rfe4 = RFE(log_model4,n_features_to_select=8)\n",
    "rfe4.fit(X_tr4a,y_tr4a)\n",
    "print(rfe4.ranking_)\n",
    "print(rfe4.support_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = X_tr4a.columns[rfe4.support_]\n",
    "X_tr4, X_t4, y_tr4, y_t4 = train_test_split(df2[cols], df2['share_cat'], test_size=0.20, \n",
    "                                                    random_state=0)\n",
    "\n",
    "\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ks_rf4 = RandomForestClassifier().fit(X_tr4,y_tr4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p_t_RF4 = ks_rf4.predict(X_t4)\n",
    "confusion_matrix(y_t4,y_p_t_RF4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(gamma='auto')\n",
    "clf.fit(X_tr1, y_tr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_t1, y_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
